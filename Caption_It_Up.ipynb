{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from glob import glob\n",
        "from tqdm.notebook import tqdm\n",
        "tqdm.pandas()\n",
        "import cv2, warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Flatten, Input, Add, Dropout, LSTM, TimeDistributed, Embedding, RepeatVector, Concatenate, Bidirectional, Convolution2D\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.utils import to_categorical, plot_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from gtts import gTTS\n",
        "import cv2\n",
        "import numpy as np\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import IPython.display as ipd"
      ],
      "metadata": {
        "id": "ijIhdPLz_Osq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTQU_Nhw_JL7"
      },
      "outputs": [],
      "source": [
        "img_path = '/content/Images/'\n",
        "images = glob(img_path+'*.jpg')\n",
        "images[:5]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(images)"
      ],
      "metadata": {
        "id": "Kr7X5ju0Fskv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "captions = open('/content/captions.txt','rb').read().decode('utf-8').split('\\n')\n",
        "captions[:5]"
      ],
      "metadata": {
        "id": "YmiX-sCP_cwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "    plt.figure(figsize=(5,5))\n",
        "    img = cv2.imread(images[i])\n",
        "    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
        "    plt.imshow(img);"
      ],
      "metadata": {
        "id": "s1qQAen_Blr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inception_model = ResNet50(include_top=True)\n",
        "inception_model.summary()"
      ],
      "metadata": {
        "id": "EP6SthOqBnpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "last = inception_model.layers[-2].output # Output of the penultimate layer of ResNet model\n",
        "model = Model(inputs=inception_model.input,outputs=last)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "zwqWTyCvBxbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_features = {}\n",
        "count = 0\n",
        "\n",
        "for img_path in tqdm(images):\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
        "    img = cv2.resize(img,(224,224)) # ResNet model requires images of dimensions (224,224,3)\n",
        "    img = img.reshape(1,224,224,3) # Reshaping image to the dimensions of a single image\n",
        "    features = model.predict(img).reshape(2048,) # Feature extraction from images\n",
        "    img_name = img_path.split('/')[-1] # Extracting image name\n",
        "    img_features[img_name] = features\n",
        "    count += 1\n",
        "    # Fetching the features of only 1500 images as using more than 1500 images leads to overloading memory issues\n",
        "    if count == 1500:\n",
        "        break\n",
        "    if count % 50 == 0:\n",
        "        print(count)"
      ],
      "metadata": {
        "id": "MgTICWvwByYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "captions = captions[1:]\n",
        "captions[:5]"
      ],
      "metadata": {
        "id": "jjmeHvLyCKOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "captions[8].split(',')[1]"
      ],
      "metadata": {
        "id": "Q1xf8IqvCONo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "captions_dict = {}\n",
        "\n",
        "for cap in captions:\n",
        "    try:\n",
        "        img_name = cap.split(',')[0]\n",
        "        caption = cap.split(',')[1]\n",
        "        # Each image has 5 captions\n",
        "        if img_name in img_features:\n",
        "            if img_name not in captions_dict:\n",
        "                captions_dict[img_name] = [caption] # Storing the first caption\n",
        "            else:\n",
        "                captions_dict[img_name].append(caption) # Adding the remaining captions\n",
        "    except:\n",
        "        break"
      ],
      "metadata": {
        "id": "1cAc0c3xCQRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def text_preprocess(text):\n",
        "    modified_text = text.lower() # Converting text to lowercase\n",
        "    modified_text = 'startofseq ' + modified_text + ' endofseq' # Appending the special tokens at the beginning and ending of text\n",
        "    return modified_text"
      ],
      "metadata": {
        "id": "tZDfwyZzCUV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Storing the preprocessed text within the captions dictionary\n",
        "for key, val in captions_dict.items():\n",
        "    for item in val:\n",
        "        captions_dict[key][val.index(item)] = text_preprocess(item)"
      ],
      "metadata": {
        "id": "kA0mbNsNCZ6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_words = dict()\n",
        "cnt = 1\n",
        "\n",
        "for key, val in captions_dict.items(): # Iterating through all images with keys as images and their values as 5 captions\n",
        "    for item in val: # Iterating through all captions for each image\n",
        "        for word in item.split(): # Iterating through all words in each caption\n",
        "            if word not in count_words:\n",
        "                count_words[word] = cnt\n",
        "                cnt += 1"
      ],
      "metadata": {
        "id": "hD33So4KCc94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding the text by assigning each word to its corresponding index in the vocabulary i.e. count_words dictionary\n",
        "for key, val in captions_dict.items():\n",
        "    for caption in val:\n",
        "        encoded = []\n",
        "        for word in caption.split():\n",
        "            encoded.append(count_words[word])\n",
        "        captions_dict[key][val.index(caption)] = encoded"
      ],
      "metadata": {
        "id": "lQsH3g-VCi9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determining the maximum possible length of text within the entire captions text corpus\n",
        "max_len = -1\n",
        "\n",
        "for key, value in captions_dict.items():\n",
        "    for caption in value:\n",
        "        if max_len < len(caption):\n",
        "            max_len = len(caption)"
      ],
      "metadata": {
        "id": "E9ao7028Cmi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "vocab_size = len(count_words) # Vocab size is the total number of words present in count_words dictionary\n",
        "vocab_size"
      ],
      "metadata": {
        "id": "qeHzyX9KCqlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generator(img,caption):\n",
        "    n_samples = 0\n",
        "    X = []\n",
        "    y_input = []\n",
        "    y_output = []\n",
        "\n",
        "    for key, val in caption.items():\n",
        "        for item in val:\n",
        "            for i in range(1,len(item)):\n",
        "                X.append(img[key]) # Appending the input image features\n",
        "                input_seq = [item[:i]] # Previously generated text to be used as input to predict the next word\n",
        "                output_seq = item[i] # The next word to be predicted as output\n",
        "                # Padding encoded text sequences to the maximum length\n",
        "                input_seq = pad_sequences(input_seq,maxlen=max_len,padding='post',truncating='post')[0]\n",
        "                # One Hot encoding the output sequence with vocabulary size as the total no. of classes\n",
        "                output_seq = to_categorical([output_seq],num_classes=vocab_size+1)[0]\n",
        "                y_input.append(input_seq)\n",
        "                y_output.append(output_seq)\n",
        "\n",
        "    return X, y_input, y_output"
      ],
      "metadata": {
        "id": "Em8jui3_Gk7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y_in, y_out = generator(img_features,captions_dict)\n",
        "\n"
      ],
      "metadata": {
        "id": "QNd_jVkgGow0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X), len(y_in), len(y_out)"
      ],
      "metadata": {
        "id": "sua-lqEUGxOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Converting input and output into Numpy arrays for faster processing\n",
        "X = np.array(X)\n",
        "y_in = np.array(y_in,dtype='float64')\n",
        "y_out = np.array(y_out,dtype='float64')"
      ],
      "metadata": {
        "id": "RaWOOQZkGx6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape, y_in.shape, y_out.shape"
      ],
      "metadata": {
        "id": "ZWBhNIElG1_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "embedding_len = 128\n",
        "MAX_LEN = max_len\n",
        "vocab_size = len(count_words)\n",
        "\n",
        "# Model for image feature extraction\n",
        "img_model = Sequential()\n",
        "img_model.add(Dense(embedding_len,input_shape=(2048,),activation='relu'))\n",
        "img_model.add(RepeatVector(MAX_LEN))\n",
        "\n",
        "img_model.summary()\n",
        "\n",
        "# Model for generating captions from image features\n",
        "captions_model = Sequential()\n",
        "captions_model.add(Embedding(input_dim=vocab_size+1,output_dim=embedding_len,input_length=MAX_LEN))\n",
        "captions_model.add(LSTM(256,return_sequences=True))\n",
        "captions_model.add(TimeDistributed(Dense(embedding_len)))\n",
        "\n",
        "captions_model.summary()\n",
        "\n",
        "# Concatenating the outputs of image and caption models\n",
        "concat_output = Concatenate()([img_model.output,captions_model.output])\n",
        "# First LSTM Layer\n",
        "output = LSTM(units=128,return_sequences=True)(concat_output)\n",
        "# Second LSTM Layer\n",
        "output = LSTM(units=512,return_sequences=False)(output)\n",
        "# Output Layer\n",
        "output = Dense(units=vocab_size+1,activation='softmax')(output)\n",
        "# Creating the final model\n",
        "final_model = Model(inputs=[img_model.input,captions_model.input],outputs=output)\n",
        "final_model.compile(loss='categorical_crossentropy',optimizer='RMSprop',metrics='accuracy')\n",
        "final_model.summary()"
      ],
      "metadata": {
        "id": "b6j_xQIjG4p-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plot_model(final_model,'model.png',show_shapes=True,dpi=100)"
      ],
      "metadata": {
        "id": "Pk6QEps-HKW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "mc = ModelCheckpoint('image_caption_generator.h5',monitor='accuracy',verbose=1,mode='max',save_best_only=True)\n",
        "\n",
        "final_model.fit([X,y_in],\n",
        "                y_out,\n",
        "                batch_size=512,\n",
        "                callbacks=mc,\n",
        "                epochs=200)"
      ],
      "metadata": {
        "id": "VvVEcT2yHP0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Creating an inverse dictionary with reverse key-value pairs\n",
        "inverse_dict = {val: key for key,val in count_words.items()}"
      ],
      "metadata": {
        "id": "Ad1fBjCCHX_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_model.save('image_caption_generator.h5')"
      ],
      "metadata": {
        "id": "krU5w1J8HaXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "np.save('vocab.npy',count_words)"
      ],
      "metadata": {
        "id": "XxZkW_26HdH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getImage(idx):\n",
        "    test_img_path = images[idx]\n",
        "    test_img = cv2.imread(test_img_path)\n",
        "    test_img = cv2.cvtColor(test_img,cv2.COLOR_BGR2RGB)\n",
        "    test_img = cv2.resize(test_img,(224,224))\n",
        "    test_img = np.reshape(test_img,(1,224,224,3))\n",
        "    return test_img"
      ],
      "metadata": {
        "id": "ztgoRKeLIdOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    random_no = np.random.randint(0, 1501, (1, 1))[0, 0]\n",
        "    test_feature = model.predict(getImage(random_no)).reshape(1, 2048)\n",
        "    test_img_path = images[random_no]\n",
        "    test_img = cv2.imread(test_img_path)\n",
        "    test_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n",
        "    pred_text = ['startofseq']\n",
        "    count = 0\n",
        "    caption = ''  # Stores the predicted captions text\n",
        "\n",
        "    # Caption Generation Loop\n",
        "    while count < 25:\n",
        "        count += 1\n",
        "        # Encoding the captions text with numbers\n",
        "        encoded = [count_words[i] for i in pred_text]\n",
        "        encoded = [encoded]\n",
        "        # Padding the encoded text sequences to maximum length\n",
        "        encoded = pad_sequences(encoded, maxlen=MAX_LEN, padding='post', truncating='post')\n",
        "        pred_idx = np.argmax(final_model.predict([test_feature, encoded]))  # Fetching the predicted word index having the maximum probability of occurrence\n",
        "        sampled_word = inverse_dict[pred_idx]  # Extracting the predicted word by its respective index\n",
        "        # Checking for ending of the sequence\n",
        "        if sampled_word == 'endofseq':\n",
        "            break\n",
        "        caption = caption + ' ' + sampled_word\n",
        "        pred_text.append(sampled_word)\n",
        "\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    plt.imshow(test_img)\n",
        "    plt.xlabel(caption)\n",
        "    plt.show()\n",
        "    tts = gTTS(caption)\n",
        "    tts.save(\"caption.mp3\")\n",
        "    ipd.display(ipd.Audio(\"caption.mp3\", autoplay=True))"
      ],
      "metadata": {
        "id": "qCyPGlbRIghV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}